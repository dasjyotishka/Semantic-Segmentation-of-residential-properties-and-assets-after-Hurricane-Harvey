{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dasjyotishka/Semantic-Segmentation-of-residential-properties-and-assets-after-Hurricane-Harvey/blob/main/Hurricane_Harvey_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hurricane Harvey Segmentation Challenge"
      ],
      "metadata": {
        "id": "7-gxKiD6K2ZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing required libraries"
      ],
      "metadata": {
        "id": "10Hf9EaBAY7x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPzmOfEd9cs8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Created on Fri Jan 13 12:23:05 2023\n",
        "\n",
        "@author: Jyotishka Das and Hao Xu\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "'''In the first section we are defining some modules that would be used in the execution phase'''\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from tensorflow import keras\n",
        "from PIL import Image, ImageOps\n",
        "from os import listdir , path\n",
        "from os.path import isfile, join\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "import glob\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "import imageio\n",
        "!pip install -q -U segmentation-models-pytorch albumentations > /dev/null\n",
        "import segmentation_models_pytorch as smp\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-processing of the images"
      ],
      "metadata": {
        "id": "Ngq4AxmeAzyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training images downloaded from the competition website are stored in folder called ***train_images***, and the corresponding masks are stored in a folder called train_masks. Two blank folders are also created in the same directory called test_images. Now, for every item in ***train_images***, we are checking whether its corresponding mask exits in ***train_mask***. If it does not exist, we move the image from ***train_images*** folder to the *test_images* folder."
      ],
      "metadata": {
        "id": "eJVErZ9YBJAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to google drive where we store the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoN14UyqCbje",
        "outputId": "b185d4fb-11be-4ba9-cec4-9c8faf827304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if gpu is connected\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6IrgmU6CkbD",
        "outputId": "540eb2a1-42c1-4dd0-ae0d-94c8887cc39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan 22 15:50:51 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    26W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 320\n",
        "\n",
        "\n",
        "train_img_dir='/content/drive/MyDrive/FDL/Final_Submission/train_images'\n",
        "train_mask_dir='/content/drive/MyDrive/FDL/Final_Submission/train_masks'\n",
        "test_img_dir='/content/drive/MyDrive/FDL/Final_Submission/test_images'\n",
        "validation_img_dir='/content/drive/MyDrive/FDL/Final_Submission/val_images'\n",
        "validation_mask_dir='/content/drive/MyDrive/FDL/Final_Submission/val_masks'\n",
        "\n",
        "files_train_img_dir = [f for f in listdir(train_img_dir) if isfile(join(train_img_dir, f))]\n",
        "files_train_mask_dir = [f for f in listdir(train_mask_dir) if isfile(join(train_mask_dir, f))]\n",
        "files_test_img_dir = [f for f in listdir(test_img_dir) if isfile(join(test_img_dir, f))]\n",
        "files_validation_img_dir = [f for f in listdir(validation_img_dir) if isfile(join(test_img_dir, f))]\n",
        "files_validation_mask_dir = [f for f in listdir(validation_mask_dir) if isfile(join(test_img_dir, f))]\n",
        "\n",
        "transform_mask_resize = T.Resize((IMG_SIZE,IMG_SIZE), interpolation=Image.NEAREST)\n",
        "transform_toTensor = T.ToTensor()\n",
        "\n",
        "\n",
        "train_dataloader=[]\n",
        "val_dataloader=[]\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buGOwDf1AyAB",
        "outputId": "eb55a527-0711-4f2f-e33b-603bcff9c52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to split the dataset into train and test datasets.\n",
        "def create_train_test_split ():\n",
        "\n",
        "\n",
        "    common_files = []\n",
        "    flag=0\n",
        "    for i in files_train_img_dir:\n",
        "        for j in files_train_mask_dir:\n",
        "            if(path.splitext(i)[0] == path.splitext(j)[0]): #this compares it name by name.\n",
        "                flag=1\n",
        "                common_files.append(i)\n",
        "        if (flag==0):\n",
        "            Path(train_img_dir+\"/\"+path.splitext(i)[0]+\".tif\").rename(test_img_dir+\"/\"+path.splitext(i)[0]+\".tif\")\n",
        "        flag=0\n",
        "    return 0\n",
        "\n",
        "\n",
        "create_train_test_split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfwUMXq4Braj",
        "outputId": "f6bb1096-36b2-431d-d6fc-cbf5821abb99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the ***train_images*** folder would contain 299 images and the ***test_images*** folder would contain 75 images. For training the model, we would need to further split the training dataset into train and validation dataset. So, we move 15% of the images (45 random images) and their corresponding masks from the training folders to the validation folders. Now, at this stage, the ***train_images*** and ***train_masks*** folders would contain 254 images, and the ***validation_images*** and ***validation_masks*** folders would contain 45 images. Create blank folders where the tensors would be stored and store the corresponding tensors in those folders. Iterate through all masks and images in train set, resize them, transform them into tensors and store them in the new folder"
      ],
      "metadata": {
        "id": "IEwrIjN3CEHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify paths to your files (glob glob stores paths to all files in a folder in a list)\n",
        "\n",
        "train_img_link_list  = sorted(glob.glob('/content/drive/MyDrive/FDL/Final_Submission/train_images/*'))\n",
        "train_mask_link_list = sorted(glob.glob('/content/drive/MyDrive/FDL/Final_Submission/train_masks/*'))\n",
        "test_img_link_list   = sorted(glob.glob('/content/drive/MyDrive/FDL/Final_Submission/test_images/*'))\n",
        "validation_img_link_list = sorted(glob.glob('/content/drive/MyDrive/FDL/Final_Submission/val_images/*'))\n",
        "validation_mask_link_list = sorted(glob.glob('/content/drive/MyDrive/FDL/Final_Submission/val_masks/*'))"
      ],
      "metadata": {
        "id": "IWmpEEnyPrew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "#Function to convert the images to tensors\n",
        "def convert_tensor ():\n",
        "\n",
        "  val_img_list=[]\n",
        "  val_img_list = [str(x) for x in val_img_list]\n",
        "\n",
        "  # Transformation pipeline\n",
        "  transform_img = transforms.Compose([\n",
        "      transforms.Resize([IMG_SIZE,IMG_SIZE]),\n",
        "      transforms.ToTensor()\n",
        "  ])\n",
        "\n",
        "  transform_mask = transforms.Compose([\n",
        "      transforms.Resize([IMG_SIZE,IMG_SIZE], interpolation=Image.NEAREST),\n",
        "      transforms.ToTensor()\n",
        "  ])\n",
        "\n",
        "  transform_mask_prova = transforms.Compose([\n",
        "      transforms.ToTensor()\n",
        "  ])\n",
        "\n",
        "  # Iterate through all masks and images in train set, transform them and store corresponging tensors in a new folder\n",
        "  for i in tqdm(range(len(train_img_link_list))):\n",
        "\n",
        "      img_id  = train_img_link_list[i].split('/')[-1].split('.')[0]\n",
        "      mask_id = train_mask_link_list[i].split('/')[-1].split('.')[0]\n",
        "      assert img_id == mask_id    # Make sure id's match\n",
        "\n",
        "      img   = Image.open(train_img_link_list[i])\n",
        "      mask  = Image.open(train_mask_link_list[i])\n",
        "\n",
        "      img  = transform_img(img)\n",
        "      mask = transform_mask(mask)*255\n",
        "      mask = mask.int()\n",
        "\n",
        "      torch.save(img,\"/content/drive/MyDrive/FDL/Final_Submission/train_images_\"+str(IMG_SIZE)+\"/\"+img_id+\".pt\")\n",
        "      torch.save(mask, \"/content/drive/MyDrive/FDL/Final_Submission/train_masks_\"+str(IMG_SIZE)+\"/\"+mask_id+\".pt\")\n",
        "\n",
        "\n",
        "  # Iterate through all masks and images in validation set, transform them and store corresponging tensors in a new folder\n",
        "  for i in tqdm(range(len(validation_img_link_list))):\n",
        "\n",
        "      img_id  = train_img_link_list[i].split('/')[-1].split('.')[0]\n",
        "      mask_id = train_mask_link_list[i].split('/')[-1].split('.')[0]\n",
        "      assert img_id == mask_id    # Make sure id's match\n",
        "\n",
        "      img   = Image.open(train_img_link_list[i])\n",
        "      mask  = Image.open(train_mask_link_list[i])\n",
        "\n",
        "      img  = transform_img(img)\n",
        "      mask = transform_mask(mask)*255\n",
        "      mask = mask.int()\n",
        "\n",
        "      torch.save(img,\"/content/drive/MyDrive/FDL/Final_Submission/val_images_\"+str(IMG_SIZE)+\"/\"+img_id+\".pt\")\n",
        "      torch.save(mask, \"/content/drive/MyDrive/FDL/Final_Submission/val_masks_\"+str(IMG_SIZE)+\"/\"+mask_id+\".pt\")\n",
        "\n",
        "  print('Train and Validation sets created')\n",
        "\n",
        "  # Iterate through all images in test set, transform them and store corresponging tensors in a new folder\n",
        "  for i in tqdm(range(len(test_img_link_list))):\n",
        "\n",
        "      img_id  = test_img_link_list[i].split('/')[-1].split('.')[0]\n",
        "\n",
        "      img   = Image.open(test_img_link_list[i])\n",
        "      img   = transform_img(img)\n",
        "\n",
        "      torch.save(img,\"/content/drive/MyDrive/FDL/Final_Submission/test_images_\"+str(IMG_SIZE)+\"/\"+img_id+\".pt\")\n",
        "\n",
        "\n",
        "  print('Test set created')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "convert_tensor()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiCUW74NB_Ff",
        "outputId": "096faf44-0d81-4361-8139-c4c7df5dda71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 254/254 [01:23<00:00,  3.03it/s]\n",
            "100%|██████████| 45/45 [00:14<00:00,  3.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train and Validation sets created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 75/75 [00:20<00:00,  3.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build a custom Dataloader"
      ],
      "metadata": {
        "id": "f8BpkWYqDKrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the image and the mask tensors. We then convert the grayscale mask tensors from the dimension [1, H, W] to [H, W] because the loss function accepts images in the form [B, H, W], where **B** is the batch-size, **H** is the height and **W** is the width of the images. After that, an ad-hoc transformation on the images are performed to improve the segmentation accuracy. If the images are too bright or too dark, then we scale its brightness.\n"
      ],
      "metadata": {
        "id": "oPEh8Kn3DrMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=10\n",
        "\n",
        "'''A class for Dataset is created'''\n",
        "\n",
        "class tensorDataset(Dataset):\n",
        "\n",
        "\n",
        "    def __init__(self, images:list, masks:list, train:bool):\n",
        "        self.image_links = images\n",
        "        self.mask_links  = masks\n",
        "        self.train = train\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Select the link to the specific image\n",
        "        img_id  = self.image_links[index]\n",
        "        mask_id = self.mask_links[index]\n",
        "\n",
        "        # Loading the image tensors\n",
        "        img  = torch.load(img_id)\n",
        "        mask = torch.load(mask_id)\n",
        "\n",
        "\n",
        "        # Convert the grayscale  mask tensors from the dimension [1, H, W] to [H, W] because the loss function accepts images in the form [B, H, W]\n",
        "        mask = mask.squeeze(0)\n",
        "\n",
        "        # Gradient is turned on for the image\n",
        "        img = img.detach().clone().requires_grad_(True)\n",
        "        mask = mask.long()\n",
        "\n",
        "        return img, mask\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_links)\n",
        "\n",
        "\n",
        "    def transform(self, img, mask):\n",
        "\n",
        "        # If the images are too bright or too dark, scale its brightness\n",
        "        temp_rand = np.random.rand()\n",
        "        if temp_rand < 0.3:\n",
        "            t_darken_image = T.ColorJitter(brightness=[0.6, 0.8])\n",
        "            img = t_darken_image(img)\n",
        "\n",
        "        elif temp_rand > 0.7:\n",
        "            t_brighten_image = T.ColorJitter(brightness=[1.2, 1.4])\n",
        "            img = t_brighten_image(img)\n",
        "\n",
        "        return img, mask"
      ],
      "metadata": {
        "id": "ITIYz31kDOT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''The train, test and validation datasets are created through DataLoaders'''\n",
        "\n",
        "list_train_img  = sorted(glob.glob('/content/drive/MyDrive/FDL/Final_Submission/train_images_320/*'))\n",
        "list_train_mask = sorted(glob.glob('/content/drive/MyDrive/FDL/Final_Submission/train_masks_320/*'))\n",
        "list_val_img    = sorted(glob.glob('/content/drive/MyDrive/FDL/Final_Submission/val_images_320/*'))\n",
        "list_val_mask   = sorted(glob.glob('/content/drive/MyDrive/FDL/Final_Submission/val_masks_320/*'))\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = tensorDataset(list_train_img, list_train_mask, train=True)\n",
        "val_dataset   = tensorDataset(list_val_img, list_val_mask, train=False)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader   = DataLoader(val_dataset  , batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "yMdbQeaLDT_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calculation of Dice-Score"
      ],
      "metadata": {
        "id": "t8p0AWNdEkpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Dice score is used to calculate the accuracy of our segmentation.\n",
        "The Dice Coefficient is 2 x (the Area of Overlap divided by the total number of pixels in the test and the predicted masks)."
      ],
      "metadata": {
        "id": "FTPV7jIRErCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#A function to calculation the Dice scores of the segmented images.\n",
        "\n",
        "def calculate_dice_score(preds, true_mask):\n",
        "\n",
        "    f1_batch = []\n",
        "\n",
        "    for i in range(len(preds)):\n",
        "        f1_image = []\n",
        "        img  = preds[i].to(DEVICE)\n",
        "        mask = true_mask[i].to(DEVICE)\n",
        "\n",
        "        # The shape of the image tensor is changed from the dimension [26, H, W] to [H, W]\n",
        "        img = torch.argmax(img, dim=0)\n",
        "\n",
        "        for label in range(26):\n",
        "            if torch.sum(mask == label) != 0:\n",
        "                area_of_intersect = torch.sum((img == label) * (mask == label))\n",
        "                area_of_img       = torch.sum(img == label)\n",
        "                area_of_label     = torch.sum(mask == label)\n",
        "                f1 = 2*area_of_intersect / (area_of_img + area_of_label)\n",
        "                f1_image.append(f1)\n",
        "\n",
        "        f1_batch.append(np.mean([tensor.cpu() for tensor in f1_image]))\n",
        "    return np.mean(f1_batch)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TRGlgxyXE2jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "def accuracy(preds, true_mask):\n",
        "    '''\n",
        "    preds should be [B, 25, H, W]\n",
        "    true_mask should be [B, H, W]\n",
        "    '''\n",
        "    accuracy_batch = []\n",
        "\n",
        "    for i in range(len(preds)):\n",
        "        img  = preds[i].to(DEVICE)\n",
        "        mask = true_mask[i].to(DEVICE)\n",
        "\n",
        "        # Change shape of img from [25, H, W] to [H, W]\n",
        "        img  = torch.argmax(img, dim=0)\n",
        "\n",
        "        accuracy_batch.append(torch.sum(img == mask).item() / (IMG_SIZE*IMG_SIZE))  # FIX LATER\n",
        "\n",
        "    return np.mean(accuracy_batch)"
      ],
      "metadata": {
        "id": "9PFx2ooTXAfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training the model"
      ],
      "metadata": {
        "id": "abwOjBGJFlHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''A function to train the model. If at any step the f1_score obtained is greater than 0.3, the model\n",
        "is saved in a directory folder.'''\n",
        "def train():\n",
        "\n",
        "  min_val_f1 = 0.3\n",
        "\n",
        "  for epoch in range(1, Epochs):\n",
        "\n",
        "      # Define evaluation metrics\n",
        "      model.train()\n",
        "      train_losses   = []\n",
        "      train_accuracy = []\n",
        "      train_f1       = []\n",
        "\n",
        "      for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "          '''Extract the data and the corresponding masks.\n",
        "          A batch of B color images of the size 512x512 are stored in a 4D-tensor of shape\n",
        "          [B,3,H,W], while a batch of B grayscale masks of the size 512x512 are\n",
        "          in the shape [B,H,W]'''\n",
        "          img_batch, mask_batch = batch\n",
        "          img_batch = img_batch.to(DEVICE)\n",
        "          mask_batch = mask_batch.to(DEVICE)\n",
        "\n",
        "          '''Train the model.'''\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          '''The output masks should be one-hot encoded\n",
        "          for every 26 output classes and would have a shape of [B, 26, H, W].\n",
        "          This is because the loss function only takes input in this format.\n",
        "          Ref: https://www.jeremyjordan.me/semantic-segmentation/'''\n",
        "          output = model(img_batch)\n",
        "          loss   = criterion(output, mask_batch)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          #After 1 epoch, add the loss to the list list\n",
        "          f1 = calculate_dice_score(output, mask_batch)\n",
        "          acc = accuracy(output, mask_batch)\n",
        "          train_losses.append(loss.item())\n",
        "          train_accuracy.append(acc)\n",
        "          train_f1.append(f1)\n",
        "\n",
        "\n",
        "      # Update the global metrics\n",
        "      print(f'TRAIN       Epoch: {epoch} | Epoch metrics | loss: {np.mean(train_losses):.4f}, f1: {np.mean(train_f1):.3f}, accuracy: {np.mean(train_accuracy):.3f}')\n",
        "      total_train_losses.append(np.mean(train_losses))\n",
        "      total_train_accuracy.append(np.mean(train_accuracy))\n",
        "      total_train_f1.append(np.mean(train_f1))\n",
        "\n",
        "\n",
        "      # Define the validate metrics\n",
        "      model.eval()\n",
        "      val_losses   = []\n",
        "      val_accuracy = []\n",
        "      val_f1       = []\n",
        "\n",
        "      for i, batch in enumerate(val_dataloader):\n",
        "          #Extract the images and the corresponding masks\n",
        "          img_batch, mask_batch = batch\n",
        "          img_batch = img_batch.to(DEVICE)\n",
        "          mask_batch = mask_batch.to(DEVICE)\n",
        "\n",
        "          #Validate model\n",
        "          with torch.cuda.amp.autocast():\n",
        "            output = model(img_batch)\n",
        "            loss   = criterion(output, mask_batch)\n",
        "\n",
        "           #After 1 epoch, add the loss to the list list\n",
        "          f1 = calculate_dice_score(output, mask_batch)\n",
        "          acc = accuracy(output, mask_batch)\n",
        "          val_losses.append(loss.item())\n",
        "          val_accuracy.append(acc)\n",
        "          val_f1.append(f1)\n",
        "\n",
        "\n",
        "\n",
        "      # Print the updated global metrics\n",
        "      print(f'VALIDATION  Epoch: {epoch} | Epoch metrics | loss: {np.mean(val_losses):.4f}, f1: {np.mean(val_f1):.3f}, accuracy: {np.mean(val_accuracy):.3f}')\n",
        "      print('---------------------------------------------------------------------------------')\n",
        "      total_val_losses.append(np.mean(val_losses))\n",
        "      total_val_accuracy.append(np.mean(val_accuracy))\n",
        "      total_val_f1.append(np.mean(val_f1))\n",
        "\n",
        "\n",
        "      # Save the model\n",
        "      if np.mean(val_f1) > min_val_f1:\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/FDL/Final_Submission/models/RESNET101-ImageNet-DeepLabV3/DeepNet_res101_SI320_EP\"+str(epoch)+\"_LR001.pt\")\n",
        "        min_val_f1 = np.mean(val_f1)\n",
        "\n",
        "      # Save the results so far\n",
        "      temp_df = pd.DataFrame(list(zip(total_train_losses, total_val_losses, total_train_f1, total_val_f1,\n",
        "                                    total_train_accuracy, total_val_accuracy)),\n",
        "                            columns = ['train_loss', 'val_loss', 'train_f1', 'test_f1', 'train_accuracy',\n",
        "                                      'test_accuracy'])\n",
        "      temp_df.to_csv('train_val_measures')"
      ],
      "metadata": {
        "id": "VjsYL6FpFfIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used a DeepLabv3+ architecture to train our semantic segmentation model. The DeepLabv3+ is a semantic segmentation architecture that improves upon DeepLabv3 with several improvements, such as adding a simple yet effective decoder module to cachieve an encoder-decoder structure. The encoder module processes multiscale contextual information by applying dilated convolution at multiple scales, while the decoder module refines the segmentation results along object boundaries.cA resnet101 architecture was found to be the best encoder for the current segmentation challenge,and it has been pretrained on the imagenet dataset."
      ],
      "metadata": {
        "id": "m6mYzqcQFxgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialization of the model and the hyperparameters.\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "\n",
        "model = smp.DeepLabV3Plus(\n",
        "     encoder_name = 'resnet101',\n",
        "     encoder_weights = 'imagenet',\n",
        "     classes = 27,\n",
        "     activation = None,\n",
        " ).to(DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "total_train_losses   = []\n",
        "total_val_losses     = []\n",
        "total_train_accuracy = []\n",
        "total_val_accuracy   = []\n",
        "total_train_f1       = []\n",
        "total_val_f1         = []"
      ],
      "metadata": {
        "id": "BO5DLcAyFvg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation Hyperparameters\n",
        "Epochs = 50\n",
        "LR = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "\n",
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWvb7HlwGB1g",
        "outputId": "2bac3795-ab89-4530-ae01-129aa8703f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN       Epoch: 1 | Epoch metrics | loss: 1.2518, f1: 0.320, accuracy: 0.708\n",
            "VALIDATION  Epoch: 1 | Epoch metrics | loss: 1.1164, f1: 0.328, accuracy: 0.744\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 2 | Epoch metrics | loss: 1.0473, f1: 0.355, accuracy: 0.741\n",
            "VALIDATION  Epoch: 2 | Epoch metrics | loss: 0.9066, f1: 0.346, accuracy: 0.770\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 3 | Epoch metrics | loss: 0.9101, f1: 0.378, accuracy: 0.767\n",
            "VALIDATION  Epoch: 3 | Epoch metrics | loss: 0.8158, f1: 0.353, accuracy: 0.784\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 4 | Epoch metrics | loss: 0.7951, f1: 0.395, accuracy: 0.792\n",
            "VALIDATION  Epoch: 4 | Epoch metrics | loss: 0.6990, f1: 0.373, accuracy: 0.814\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 5 | Epoch metrics | loss: 0.7195, f1: 0.414, accuracy: 0.810\n",
            "VALIDATION  Epoch: 5 | Epoch metrics | loss: 0.6425, f1: 0.386, accuracy: 0.822\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 6 | Epoch metrics | loss: 0.6471, f1: 0.422, accuracy: 0.827\n",
            "VALIDATION  Epoch: 6 | Epoch metrics | loss: 0.6198, f1: 0.390, accuracy: 0.824\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 7 | Epoch metrics | loss: 0.6261, f1: 0.429, accuracy: 0.828\n",
            "VALIDATION  Epoch: 7 | Epoch metrics | loss: 0.5730, f1: 0.395, accuracy: 0.836\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 8 | Epoch metrics | loss: 0.6108, f1: 0.437, accuracy: 0.833\n",
            "VALIDATION  Epoch: 8 | Epoch metrics | loss: 0.5719, f1: 0.391, accuracy: 0.832\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 9 | Epoch metrics | loss: 0.5815, f1: 0.438, accuracy: 0.836\n",
            "VALIDATION  Epoch: 9 | Epoch metrics | loss: 0.5296, f1: 0.404, accuracy: 0.845\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 10 | Epoch metrics | loss: 0.5371, f1: 0.448, accuracy: 0.846\n",
            "VALIDATION  Epoch: 10 | Epoch metrics | loss: 0.5134, f1: 0.413, accuracy: 0.848\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 11 | Epoch metrics | loss: 0.5136, f1: 0.453, accuracy: 0.849\n",
            "VALIDATION  Epoch: 11 | Epoch metrics | loss: 0.4772, f1: 0.416, accuracy: 0.855\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 12 | Epoch metrics | loss: 0.5148, f1: 0.455, accuracy: 0.847\n",
            "VALIDATION  Epoch: 12 | Epoch metrics | loss: 0.4777, f1: 0.419, accuracy: 0.857\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 13 | Epoch metrics | loss: 0.4752, f1: 0.463, accuracy: 0.859\n",
            "VALIDATION  Epoch: 13 | Epoch metrics | loss: 0.4449, f1: 0.435, accuracy: 0.864\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 14 | Epoch metrics | loss: 0.4562, f1: 0.477, accuracy: 0.865\n",
            "VALIDATION  Epoch: 14 | Epoch metrics | loss: 0.4222, f1: 0.446, accuracy: 0.869\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 15 | Epoch metrics | loss: 0.4213, f1: 0.489, accuracy: 0.874\n",
            "VALIDATION  Epoch: 15 | Epoch metrics | loss: 0.3969, f1: 0.458, accuracy: 0.876\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 16 | Epoch metrics | loss: 0.4098, f1: 0.490, accuracy: 0.876\n",
            "VALIDATION  Epoch: 16 | Epoch metrics | loss: 0.3857, f1: 0.462, accuracy: 0.879\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 17 | Epoch metrics | loss: 0.3944, f1: 0.497, accuracy: 0.880\n",
            "VALIDATION  Epoch: 17 | Epoch metrics | loss: 0.3656, f1: 0.474, accuracy: 0.884\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 18 | Epoch metrics | loss: 0.3779, f1: 0.503, accuracy: 0.884\n",
            "VALIDATION  Epoch: 18 | Epoch metrics | loss: 0.3564, f1: 0.476, accuracy: 0.885\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 19 | Epoch metrics | loss: 0.3604, f1: 0.510, accuracy: 0.888\n",
            "VALIDATION  Epoch: 19 | Epoch metrics | loss: 0.3431, f1: 0.485, accuracy: 0.888\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 20 | Epoch metrics | loss: 0.3824, f1: 0.510, accuracy: 0.882\n",
            "VALIDATION  Epoch: 20 | Epoch metrics | loss: 0.3558, f1: 0.483, accuracy: 0.884\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 21 | Epoch metrics | loss: 0.3725, f1: 0.511, accuracy: 0.882\n",
            "VALIDATION  Epoch: 21 | Epoch metrics | loss: 0.3575, f1: 0.472, accuracy: 0.882\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 22 | Epoch metrics | loss: 0.3537, f1: 0.518, accuracy: 0.887\n",
            "VALIDATION  Epoch: 22 | Epoch metrics | loss: 0.3386, f1: 0.497, accuracy: 0.888\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 23 | Epoch metrics | loss: 0.3397, f1: 0.524, accuracy: 0.892\n",
            "VALIDATION  Epoch: 23 | Epoch metrics | loss: 0.3186, f1: 0.502, accuracy: 0.894\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 24 | Epoch metrics | loss: 0.3280, f1: 0.532, accuracy: 0.896\n",
            "VALIDATION  Epoch: 24 | Epoch metrics | loss: 0.3102, f1: 0.510, accuracy: 0.896\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 25 | Epoch metrics | loss: 0.3109, f1: 0.539, accuracy: 0.900\n",
            "VALIDATION  Epoch: 25 | Epoch metrics | loss: 0.2984, f1: 0.508, accuracy: 0.900\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 26 | Epoch metrics | loss: 0.3001, f1: 0.546, accuracy: 0.903\n",
            "VALIDATION  Epoch: 26 | Epoch metrics | loss: 0.2897, f1: 0.523, accuracy: 0.902\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 27 | Epoch metrics | loss: 0.2876, f1: 0.555, accuracy: 0.906\n",
            "VALIDATION  Epoch: 27 | Epoch metrics | loss: 0.2814, f1: 0.522, accuracy: 0.904\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 28 | Epoch metrics | loss: 0.2829, f1: 0.557, accuracy: 0.907\n",
            "VALIDATION  Epoch: 28 | Epoch metrics | loss: 0.2783, f1: 0.536, accuracy: 0.905\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 29 | Epoch metrics | loss: 0.2800, f1: 0.563, accuracy: 0.909\n",
            "VALIDATION  Epoch: 29 | Epoch metrics | loss: 0.2660, f1: 0.547, accuracy: 0.910\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 30 | Epoch metrics | loss: 0.2752, f1: 0.566, accuracy: 0.910\n",
            "VALIDATION  Epoch: 30 | Epoch metrics | loss: 0.2627, f1: 0.555, accuracy: 0.911\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 31 | Epoch metrics | loss: 0.2703, f1: 0.568, accuracy: 0.911\n",
            "VALIDATION  Epoch: 31 | Epoch metrics | loss: 0.2553, f1: 0.560, accuracy: 0.913\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 32 | Epoch metrics | loss: 0.2712, f1: 0.571, accuracy: 0.911\n",
            "VALIDATION  Epoch: 32 | Epoch metrics | loss: 0.2501, f1: 0.565, accuracy: 0.914\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 33 | Epoch metrics | loss: 0.2672, f1: 0.572, accuracy: 0.912\n",
            "VALIDATION  Epoch: 33 | Epoch metrics | loss: 0.2530, f1: 0.561, accuracy: 0.912\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 34 | Epoch metrics | loss: 0.2543, f1: 0.582, accuracy: 0.916\n",
            "VALIDATION  Epoch: 34 | Epoch metrics | loss: 0.2420, f1: 0.572, accuracy: 0.916\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 35 | Epoch metrics | loss: 0.2572, f1: 0.581, accuracy: 0.914\n",
            "VALIDATION  Epoch: 35 | Epoch metrics | loss: 0.2537, f1: 0.569, accuracy: 0.911\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 36 | Epoch metrics | loss: 0.2573, f1: 0.581, accuracy: 0.914\n",
            "VALIDATION  Epoch: 36 | Epoch metrics | loss: 0.2380, f1: 0.570, accuracy: 0.917\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 37 | Epoch metrics | loss: 0.2471, f1: 0.586, accuracy: 0.916\n",
            "VALIDATION  Epoch: 37 | Epoch metrics | loss: 0.2314, f1: 0.577, accuracy: 0.919\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 38 | Epoch metrics | loss: 0.2380, f1: 0.597, accuracy: 0.920\n",
            "VALIDATION  Epoch: 38 | Epoch metrics | loss: 0.2255, f1: 0.584, accuracy: 0.921\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 39 | Epoch metrics | loss: 0.2338, f1: 0.598, accuracy: 0.921\n",
            "VALIDATION  Epoch: 39 | Epoch metrics | loss: 0.2270, f1: 0.583, accuracy: 0.920\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 40 | Epoch metrics | loss: 0.2373, f1: 0.597, accuracy: 0.920\n",
            "VALIDATION  Epoch: 40 | Epoch metrics | loss: 0.2296, f1: 0.575, accuracy: 0.919\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 41 | Epoch metrics | loss: 0.2310, f1: 0.600, accuracy: 0.922\n",
            "VALIDATION  Epoch: 41 | Epoch metrics | loss: 0.2225, f1: 0.593, accuracy: 0.921\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 42 | Epoch metrics | loss: 0.2234, f1: 0.603, accuracy: 0.923\n",
            "VALIDATION  Epoch: 42 | Epoch metrics | loss: 0.2155, f1: 0.594, accuracy: 0.924\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 43 | Epoch metrics | loss: 0.2208, f1: 0.607, accuracy: 0.924\n",
            "VALIDATION  Epoch: 43 | Epoch metrics | loss: 0.2075, f1: 0.595, accuracy: 0.926\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 44 | Epoch metrics | loss: 0.2172, f1: 0.613, accuracy: 0.926\n",
            "VALIDATION  Epoch: 44 | Epoch metrics | loss: 0.2086, f1: 0.608, accuracy: 0.926\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 45 | Epoch metrics | loss: 0.2148, f1: 0.613, accuracy: 0.926\n",
            "VALIDATION  Epoch: 45 | Epoch metrics | loss: 0.2015, f1: 0.607, accuracy: 0.929\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 46 | Epoch metrics | loss: 0.2132, f1: 0.620, accuracy: 0.926\n",
            "VALIDATION  Epoch: 46 | Epoch metrics | loss: 0.2003, f1: 0.617, accuracy: 0.929\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 47 | Epoch metrics | loss: 0.2194, f1: 0.617, accuracy: 0.924\n",
            "VALIDATION  Epoch: 47 | Epoch metrics | loss: 0.2412, f1: 0.603, accuracy: 0.916\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 48 | Epoch metrics | loss: 0.2362, f1: 0.612, accuracy: 0.921\n",
            "VALIDATION  Epoch: 48 | Epoch metrics | loss: 0.2233, f1: 0.605, accuracy: 0.921\n",
            "---------------------------------------------------------------------------------\n",
            "TRAIN       Epoch: 49 | Epoch metrics | loss: 0.2199, f1: 0.613, accuracy: 0.924\n",
            "VALIDATION  Epoch: 49 | Epoch metrics | loss: 0.2134, f1: 0.600, accuracy: 0.923\n",
            "---------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the train-loss and validation-loss graph\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(list(range(len(total_train_losses)+1))[:49], total_train_losses[:49])\n",
        "plt.plot(list(range(len(total_train_losses)+1))[:49], total_val_losses[:49])\n",
        "plt.legend(['train loss', 'val loss', 'val f1'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "J1JX5a9JGCim",
        "outputId": "89f7029e-9381-42a9-b513-4133c4e620c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction of masks for test images"
      ],
      "metadata": {
        "id": "Ezk2rbNpFHxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model for which we want to generate prediction masks and generate the masks. We selected the model saved at epoch 46 because it gave the highest validation accuracy (92.9%)"
      ],
      "metadata": {
        "id": "ch3AAElSGiZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = smp.DeepLabV3Plus(\n",
        "     encoder_name = 'resnet101',\n",
        "     encoder_weights = 'imagenet',\n",
        "     classes = 27,\n",
        "     activation = None,\n",
        " )\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/FDL/Final_Submission/models/RESNET101-ImageNet-DeepLabV3/DeepNet_res101_SI320_EP26_LR001.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfaWJR5eGZBJ",
        "outputId": "ff00f2a9-ca31-4f9a-f683-b6a20dfb5465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate the prediction masks and resize the mask sizes to that of their corresponding image size before submission."
      ],
      "metadata": {
        "id": "7azSuojqGNx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Function to predict the masks of the test images'''\n",
        "def predict_masks():\n",
        "    test_img_link_list = sorted(glob.glob('/content/drive/MyDrive/FDL/Final_Submission/test_images_320/*'))\n",
        "    torch_list = [torch.load(link) for link in test_img_link_list]\n",
        "    model.eval()\n",
        "    import tqdm\n",
        "    # Calculate output for each image in test set and save the prediction in new test_preds folder\n",
        "    for i in tqdm.tqdm(range(len(torch_list))):\n",
        "      img_id = test_img_link_list[i].split('/')[-1].split('.')[0]\n",
        "      img = torch_list[i].unsqueeze(0)\n",
        "      output = model(img)\n",
        "      output = torch.argmax(output, dim=1).squeeze(0)\n",
        "      output = np.uint8(output)\n",
        "      output = Image.fromarray(output)\n",
        "      output.save(\"/content/drive/MyDrive/FDL/Final_Submission/test_preds/\"+str(img_id)+\".png\")\n",
        "\n",
        "predict_masks()\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0jRi_jmFNa7",
        "outputId": "e6c4a805-c1c8-437b-8489-cb46482f4c87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 75/75 [00:55<00:00,  1.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''The masks obtained are of the size 512x512. We need to resize the sizes masks to that of the corresponding images'''\n",
        "def resize_images():\n",
        "    directory_resize=sorted(glob.glob('/content/drive/MyDrive/FDL/Final_Submission/test_preds/*'))\n",
        "    pred_names=[]\n",
        "    for numbers in directory_resize:\n",
        "      pred_names.append(numbers.split('/')[-1].split('.')[0])\n",
        "    for name_img in tqdm(pred_names):\n",
        "      raw = cv2.imread(\"/content/drive/MyDrive/FDL/Final_Submission/test_images/\"+str(name_img)+\".tif\")\n",
        "      shape = (raw.shape[1], raw.shape[0])\n",
        "      image_read = cv2.imread(\"/content/drive/MyDrive/FDL/Final_Submission/test_preds/\"+str(name_img)+\".png\", -1)\n",
        "      image_read = cv2.resize(image_read, shape, interpolation = cv2.INTER_NEAREST)\n",
        "      cv2.imwrite(\"/content/drive/MyDrive/FDL/Final_Submission/test_preds_resized/\"+str(name_img)+\".png\", image_read)\n",
        "\n",
        "resize_images()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuwlqcWGGwxG",
        "outputId": "8699d672-f758-4f55-f8e1-b48d96014ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 75/75 [00:17<00:00,  4.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Submission of tar file'''\n",
        "def submission():\n",
        "    import tarfile\n",
        "\n",
        "    tar = tarfile.open(\"DeepLabV3-Epoch-LR0001.tar\", \"w\")\n",
        "\n",
        "    for root, dir, files in os.walk('/content/drive/MyDrive/FDL/Final_Submission/test_preds_resized'):\n",
        "        for  file in files:\n",
        "            fullpath = os.path.join(root, file)\n",
        "            tar.add(fullpath, arcname=file)\n",
        "\n",
        "    tar.close()\n",
        "\n",
        "submission()\n"
      ],
      "metadata": {
        "id": "ha2eICzNG0U_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}